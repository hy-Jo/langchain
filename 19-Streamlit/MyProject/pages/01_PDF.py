import streamlit as st
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_teddynote import logging
from dotenv import load_dotenv
from langchain_core.prompts import load_prompt
import os

load_dotenv()

st.set_page_config(page_title="PDFğŸ™")
# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.
logging.langsmith("[Project] PDF RAG")

#ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")

# íŒŒì¼ ì—…ë¡œë“œ ì „ìš© í´ë”
if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")  

if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

st.title("API SpecğŸ’¬")

#ì²˜ìŒ 1ë²ˆë§Œ ì‹¤í–‰ í•˜ê¸° ìœ„í•œ ì½”ë“œ
if "messages" not in st.session_state:
    st.session_state["messages"] = [] # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥

if "chain" not in st.session_state:
    st.session_state["chain"] = None

#ì‚¬ì´ë“œë°” ìƒì„±
with st.sidebar:
     #ì´ˆê¸°í™” ë²„íŠ¼ ìƒì„±
    clear_btn = st.button("ëŒ€í™”ë‚´ìš© ì´ˆê¸°í™”")
    uploaded_files = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ",  accept_multiple_files=True, type=["PDF"])
    selected_model= st.selectbox(
        "LLM ì„ íƒ", ["gpt-4o", "gpt-4-turbo", "gpt-4o-mini"], index=0
        )
   
    # ì´ì „ ëŒ€í™” ì¶œë ¥
def print_messages():
    for msg in st.session_state["messages"]:
        st.chat_message(msg.role).write(msg.content)

# ìƒˆë¡œìš´ ë©”ì„¸ì§€ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages"].append(ChatMessage(role=role, content=message))

#íŒŒì¼ ìºì‹œ ì €ì¥ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì‘ì—…ì„ ì²˜ë¦¬í•  ì˜ˆì •)
@st.cache_resource(show_spinner="ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...") #cache ë¦¬ì†ŒìŠ¤ëŠ” ì˜¤ë˜ê±¸ë¦¬ëŠ” ì²˜ë¦¬ë¥¼ ìŠ¤í”¼ë„ˆë¥¼ ì‚¬ìš©í•´ì„œ ì§€ë£¨í•˜ì§€ ì•Šê²Œ? ë˜ëŠ” ì²˜ë¦¬ ì¤‘ì´ë¼ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´
def embed_files(files):
    all_split_documents = []
    
    for file in files:
        # ì—…ë¡œë“œí•œ íŒŒì¼ì„ ìºì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.
        file_content = file.read()
        file_path = f"./.cache/files/{file.name}"
        with open(file_path, "wb") as f:
            f.write(file_content)
        
        # ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë“œ(Load Documents)
        loader = PyMuPDFLoader(file_path)
        docs = loader.load()

        # ë‹¨ê³„ 2: ë¬¸ì„œ ë¶„í• (Split Documents)
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
        split_documents = text_splitter.split_documents(docs)
        
        all_split_documents.extend(split_documents)

    # ë‹¨ê³„ 3: ì„ë² ë”©(Embedding) ìƒì„±
    embeddings = OpenAIEmbeddings()

    # ë‹¨ê³„ 4: DB ìƒì„±(Create DB) ë° ì €ì¥
    # ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    vectorstore = FAISS.from_documents(documents=all_split_documents, embedding=embeddings)

    # ë‹¨ê³„ 5: ê²€ìƒ‰ê¸°(Retriever) ìƒì„±
    # ë¬¸ì„œì— í¬í•¨ë˜ì–´ ìˆëŠ” ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ìƒì„±í•©ë‹ˆë‹¤.
    retriever = vectorstore.as_retriever()
    
    return retriever


# ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
def create_chain(retriever, model_name="gpt-4o"):
    # ë‹¨ê³„ 6: í”„ë¡¬í”„íŠ¸ ìƒì„±(Create Prompt)
    # í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    prompt = load_prompt("prompts/pdf-rag.yaml", encoding= "utf-8")

    # ë‹¨ê³„ 7: ì–¸ì–´ëª¨ë¸(LLM) ìƒì„±
    # ëª¨ë¸(LLM) ì„ ìƒì„±í•©ë‹ˆë‹¤.
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0)

    # ë‹¨ê³„ 8: ì²´ì¸(Chain) ìƒì„±
    chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
    )
    return chain

if uploaded_files:
    #íŒŒì¼ ì—…ë¡œë“œ í›„ retriever ìƒì„±(ì˜¤ë˜ê±¸ë¦´ ì˜ˆì •)
    retriever = embed_files(uploaded_files)
    chain = create_chain(retriever, model_name=selected_model)
    st.session_state["chain"] = chain

if clear_btn:
    st.session_state["messages"] = []

#ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

#ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ê¶ê¸ˆí•œ ë‚´ìš©ì„ ë¬¼ì–´ë³´ì„¸ìš”!")

#ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆì˜ì—­
warning_message = st.empty()


#ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©°
if user_input :
   
    #chainì„ ìƒì„±
    chain = st.session_state["chain"]

    if chain is not None:
         #ì‚¬ìš©ì ì…ë ¥
        st.chat_message("user").write(user_input)
        # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ
        response = chain.stream(user_input)
        with st.chat_message("assistant"):
            container = st.empty()

            ai_answer = ""
            for token in response:
                ai_answer += token
                container.markdown(ai_answer)

        #ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.        
        add_message("user", user_input)
        add_message("ai", ai_answer)
    else :
        warning_message.error("íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”")