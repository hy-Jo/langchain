{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 예시: 프롬프트 + 모델 + 출력 파서\n",
    "\n",
    "가장 기본적이고 일반적인 사용 사례는 prompt 템플릿과 모델을 함께 연결하는 것입니다. 이것이 어떻게 작동하는지 보기 위해, 각 나라별 수도를 물어보는 Chain을 생성해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEY를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY 정보로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith 추적을 시작합니다.\n",
      "[프로젝트명]\n",
      "CH01-Basic\n"
     ]
    }
   ],
   "source": [
    "# LangSmith 추적을 설정합니다. https://smith.langchain.com\n",
    "# !pip install -qU langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# 프로젝트 이름을 입력합니다.\n",
    "logging.langsmith(\"CH01-Basic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 프롬프트 템플릿의 활용\n",
    "\n",
    "`PromptTemplate`\n",
    "\n",
    "- 사용자의 입력 변수를 사용하여 완전한 프롬프트 문자열을 만드는 데 사용되는 템플릿입니다\n",
    "- 사용법\n",
    "  - `template`: 템플릿 문자열입니다. 이 문자열 내에서 중괄호 `{}`는 변수를 나타냅니다.\n",
    "  - `input_variables`: 중괄호 안에 들어갈 변수의 이름을 리스트로 정의합니다.\n",
    "\n",
    "`input_variables`\n",
    "\n",
    "- input_variables는 PromptTemplate에서 사용되는 변수의 이름을 정의하는 리스트입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_teddynote.messages import stream_response  # 스트리밍 출력\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`from_template()` 메소드를 사용하여 PromptTemplate 객체 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['country'], input_types={}, partial_variables={}, template='{country}의 수도는 어디인가요?')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template 정의\n",
    "template = \"{country}의 수도는 어디인가요?\"\n",
    "\n",
    "# from_template 메소드를 이용하여 PromptTemplate 객체 생성\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'대한민국의 수도는 어디인가요?'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 생성\n",
    "prompt = prompt_template.format(country=\"대한민국\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'미국의 수도는 어디인가요?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 생성\n",
    "prompt = prompt_template.format(country=\"미국\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain 생성\n",
    "\n",
    "### LCEL(LangChain Expression Language)\n",
    "\n",
    "![lcel.png](./images/lcel.png)\n",
    "\n",
    "여기서 우리는 LCEL을 사용하여 다양한 구성 요소를 단일 체인으로 결합합니다\n",
    "\n",
    "```\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "\n",
    "`|` 기호는 [unix 파이프 연산자](<https://en.wikipedia.org/wiki/Pipeline_(Unix)>)와 유사하며, 서로 다른 구성 요소를 연결하고 한 구성 요소의 출력을 다음 구성 요소의 입력으로 전달합니다.\n",
    "\n",
    "이 체인에서 사용자 입력은 프롬프트 템플릿으로 전달되고, 그런 다음 프롬프트 템플릿 출력은 모델로 전달됩니다. 각 구성 요소를 개별적으로 살펴보면 무슨 일이 일어나고 있는지 이해할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 를 PromptTemplate 객체로 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\"{topic} 에 대해 {how}게 설명해주세요.\")\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.1)\n",
    "\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['how', 'topic'], input_types={}, partial_variables={}, template='{topic} 에 대해 {how}게 설명해주세요.')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002333ABEBD10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002333AC12150>, root_client=<openai.OpenAI object at 0x000002333AAF8290>, root_async_client=<openai.AsyncOpenAI object at 0x000002333ABFAA10>, model_name='gpt-4o-mini', temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### invoke() 호출\n",
    "\n",
    "- python 딕셔너리 형태로 입력값을 전달합니다.(키: 값)\n",
    "- invoke() 함수 호출 시, 입력값을 전달합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 딕셔너리에 주제를 '인공지능 모델의 학습 원리'으로 설정합니다.\n",
    "input = {\"topic\": \"인공지능 모델의 학습 원리\",  \"how\" : \"영어로\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The learning principle of artificial intelligence (AI) models primarily revolves around the concept of machine learning, which enables systems to learn from data and improve their performance over time without being explicitly programmed for every task. Here’s a brief overview of the key components involved in this process:\\n\\n1. **Data Collection**: AI models require large amounts of data to learn from. This data can be in various forms, such as images, text, or numerical values, and it is essential for training the model.\\n\\n2. **Feature Extraction**: In this step, relevant features or characteristics are identified from the raw data. This process helps the model focus on the most important aspects of the data that contribute to making predictions or decisions.\\n\\n3. **Model Selection**: Different algorithms can be used to create AI models, such as decision trees, neural networks, or support vector machines. The choice of model depends on the nature of the data and the specific task at hand.\\n\\n4. **Training**: During training, the model learns to make predictions by adjusting its internal parameters based on the input data and the corresponding output (labels). This is typically done using a process called optimization, where the model minimizes the difference between its predictions and the actual outcomes (loss function).\\n\\n5. **Validation and Testing**: After training, the model is validated using a separate dataset to ensure it generalizes well to new, unseen data. This helps to prevent overfitting, where the model performs well on training data but poorly on new data.\\n\\n6. **Deployment**: Once the model is trained and validated, it can be deployed to make predictions or decisions in real-world applications.\\n\\n7. **Continuous Learning**: Many AI systems can continue to learn and adapt over time as they are exposed to new data, allowing them to improve their performance and stay relevant.\\n\\nIn summary, the learning principle of AI models involves collecting data, extracting features, selecting appropriate algorithms, training the model, validating its performance, and deploying it for practical use, with the potential for ongoing learning and adaptation.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 412, 'prompt_tokens': 24, 'total_tokens': 436, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-56236c0c-0948-4ec7-a274-6a24f0e9b5ed-0', usage_metadata={'input_tokens': 24, 'output_tokens': 412, 'total_tokens': 436, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 객체와 model 객체를 파이프(|) 연산자로 연결하고 invoke 메서드를 사용하여 input을 전달합니다.\n",
    "# 이를 통해 AI 모델이 생성한 메시지를 반환합니다.\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 스트리밍을 출력하는 예시 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The learning principle of artificial intelligence (AI) models primarily revolves around the concept of machine learning, which enables systems to learn from data and improve their performance over time without being explicitly programmed for every task. Here’s a breakdown of the key components involved in this process:\n",
      "\n",
      "1. **Data Collection**: AI models require large amounts of data to learn from. This data can be in various forms, such as text, images, audio, or numerical values. The quality and quantity of the data significantly influence the model's performance.\n",
      "\n",
      "2. **Feature Extraction**: In many cases, raw data needs to be transformed into a format that the model can understand. This involves identifying and extracting relevant features or characteristics from the data that will help the model make predictions or decisions.\n",
      "\n",
      "3. **Model Selection**: There are various types of machine learning models, such as supervised learning, unsupervised learning, and reinforcement learning. The choice of model depends on the nature of the task and the type of data available.\n",
      "\n",
      "   - **Supervised Learning**: The model is trained on labeled data, where the input data is paired with the correct output. The model learns to map inputs to outputs.\n",
      "   - **Unsupervised Learning**: The model works with unlabeled data and tries to find patterns or groupings within the data.\n",
      "   - **Reinforcement Learning**: The model learns by interacting with an environment and receiving feedback in the form of rewards or penalties.\n",
      "\n",
      "4. **Training**: During the training phase, the model is exposed to the training data. It makes predictions and adjusts its internal parameters based on the errors it makes. This process often involves optimization algorithms, such as gradient descent, to minimize the difference between the predicted and actual outcomes.\n",
      "\n",
      "5. **Validation and Testing**: After training, the model is validated using a separate dataset to ensure it generalizes well to new, unseen data. This helps to prevent overfitting, where the model performs well on training data but poorly on new data.\n",
      "\n",
      "6. **Deployment and Continuous Learning**: Once the model is validated, it can be deployed in real-world applications. Many AI systems continue to learn and adapt over time by incorporating new data and feedback, allowing them to improve their performance continuously.\n",
      "\n",
      "In summary, the learning principle of AI models is about using data to train algorithms that can recognize patterns, make predictions, and improve their performance through experience. This process is foundational to many applications of AI, from image recognition to natural language processing."
     ]
    }
   ],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream(input)\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력파서(Output Parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain 에 출력파서를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프롬프트, 모델, 출력 파서를 연결하여 처리 체인을 구성합니다.\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['how', 'topic'], input_types={}, partial_variables={}, template='{topic} 에 대해 {how}게 설명해주세요.')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002333ABEBD10>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002333AC12150>, root_client=<openai.OpenAI object at 0x000002333AAF8290>, root_async_client=<openai.AsyncOpenAI object at 0x000002333ABFAA10>, model_name='gpt-4o-mini', temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 객체의 invoke 메서드를 사용하여 input을 전달합니다.\n",
    "input = {\"topic\": \"인공지능 모델의 학습 원리\"}\n",
    "chain.invoke(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream(input)\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 템플릿을 변경하여 적용\n",
    "\n",
    "- 아래의 프롬프트 내용을 얼마든지 **변경** 하여 테스트 해볼 수 있습니다.\n",
    "- `model_name` 역시 변경하여 테스트가 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "당신은 영어를 가르치는 10년차 영어 선생님입니다. 주어진 상황에 맞는 영어 회화를 작성해 주세요.\n",
    "양식은 [FORMAT]을 참고하여 작성해 주세요.\n",
    "\n",
    "#상황:\n",
    "{question}\n",
    "\n",
    "#FORMAT:\n",
    "- 영어 회화:\n",
    "- 한글 해석:\n",
    "\"\"\"\n",
    "\n",
    "# 프롬프트 템플릿을 이용하여 프롬프트를 생성합니다.\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# ChatOpenAI 챗모델을 초기화합니다.\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "# 문자열 출력 파서를 초기화합니다.\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인을 구성합니다.\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 영어 회화:\n",
      "  1. Customer: \"Hi, I would like to see the menu, please.\"\n",
      "  2. Waiter: \"Of course! Here you go. Can I get you something to drink while you decide?\"\n",
      "  3. Customer: \"Yes, I would like a glass of water, please.\"\n",
      "  4. Waiter: \"Sure! Are you ready to order, or do you need more time?\"\n",
      "  5. Customer: \"I think I'm ready. I would like the grilled chicken with a side salad.\"\n",
      "  6. Waiter: \"Great choice! Would you like any dressing with your salad?\"\n",
      "  7. Customer: \"Yes, please. I'll have the balsamic vinaigrette.\"\n",
      "  8. Waiter: \"Perfect! Your order will be ready shortly.\"\n",
      "\n",
      "- 한글 해석:\n",
      "  1. 손님: \"안녕하세요, 메뉴를 보고 싶어요.\"\n",
      "  2. 웨이터: \"물론이죠! 여기 있습니다. 결정하시는 동안 음료수 드릴까요?\"\n",
      "  3. 손님: \"네, 물 한 잔 주세요.\"\n",
      "  4. 웨이터: \"알겠습니다! 주문할 준비가 되셨나요, 아니면 더 필요하신가요?\"\n",
      "  5. 손님: \"이제 준비된 것 같아요. 그릴에 구운 치킨과 사이드 샐러드를 주문할게요.\"\n",
      "  6. 웨이터: \"좋은 선택입니다! 샐러드에 드레싱을 추가할까요?\"\n",
      "  7. 손님: \"네, 발사믹 비네그렛 드레싱으로 주세요.\"\n",
      "  8. 웨이터: \"완벽합니다! 주문하신 음식은 곧 준비될 거예요.\"\n"
     ]
    }
   ],
   "source": [
    "# 완성된 Chain을 실행하여 답변을 얻습니다.\n",
    "print(chain.invoke({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 영어 회화:\n",
      "  1. Customer: \"Hi, could I see the menu, please?\"\n",
      "  2. Waiter: \"Of course! Here you go.\"\n",
      "  3. Customer: \"Thank you! I think I'll have the grilled salmon.\"\n",
      "  4. Waiter: \"Great choice! Would you like any sides with that?\"\n",
      "  5. Customer: \"Yes, I'll have a side of steamed vegetables, please.\"\n",
      "  6. Waiter: \"Perfect! Anything to drink?\"\n",
      "  7. Customer: \"I'll have a glass of water, please.\"\n",
      "\n",
      "- 한글 해석:\n",
      "  1. 손님: \"안녕하세요, 메뉴 좀 보여주실 수 있나요?\"\n",
      "  2. 웨이터: \"물론입니다! 여기 있습니다.\"\n",
      "  3. 손님: \"감사합니다! 저는 구운 연어를 주문할게요.\"\n",
      "  4. 웨이터: \"좋은 선택입니다! 사이드 메뉴는 필요하신가요?\"\n",
      "  5. 손님: \"네, 찐 야채 사이드를 주세요.\"\n",
      "  6. 웨이터: \"완벽합니다! 음료는 무엇을 드릴까요?\"\n",
      "  7. 손님: \"물 한 잔 주세요.\""
     ]
    }
   ],
   "source": [
    "# 완성된 Chain을 실행하여 답변을 얻습니다.\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"question\": \"저는 식당에 가서 음식을 주문하고 싶어요\"})\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 영어 회화:\n",
      "Customer: Hi, I’d like to order a pizza, please.  \n",
      "Pizza Shop: Sure! What size would you like?  \n",
      "Customer: I’ll have a large pizza, please.  \n",
      "Pizza Shop: Great! What toppings do you want?  \n",
      "Customer: Can I get pepperoni, mushrooms, and extra cheese?  \n",
      "Pizza Shop: Absolutely! Would you like anything to drink?  \n",
      "Customer: Yes, a 2-liter bottle of cola, please.  \n",
      "Pizza Shop: Perfect! Your total comes to $25. Would you like it for delivery or pick-up?  \n",
      "Customer: I’ll pick it up. How long will it take?  \n",
      "Pizza Shop: It should be ready in about 20 minutes.  \n",
      "Customer: Thank you! See you soon.  \n",
      "Pizza Shop: You’re welcome! See you!\n",
      "\n",
      "- 한글 해석:\n",
      "고객: 안녕하세요, 피자 하나 주문하고 싶어요.  \n",
      "피자 가게: 물론이죠! 어떤 사이즈로 드릴까요?  \n",
      "고객: 큰 피자로 해주세요.  \n",
      "피자 가게: 좋습니다! 어떤 토핑을 원하시나요?  \n",
      "고객: 페퍼로니, 버섯, 그리고 치즈를 추가해 주세요.  \n",
      "피자 가게: 알겠습니다! 음료수는 필요하신가요?  \n",
      "고객: 네, 2리터 콜라 하나 주세요.  \n",
      "피자 가게: 완벽해요! 총 금액은 25달러입니다. 배달로 하실 건가요, 아니면 픽업으로 하실 건가요?  \n",
      "고객: 저는 픽업할게요. 얼마나 걸릴까요?  \n",
      "피자 가게: 약 20분 정도 소요될 것입니다.  \n",
      "고객: 감사합니다! 곧 뵙겠습니다.  \n",
      "피자 가게: 천만에요! 곧 뵙겠습니다!"
     ]
    }
   ],
   "source": [
    "# 이번에는 question 을 '미국에서 피자 주문'으로 설정하여 실행합니다.\n",
    "# 스트리밍 출력을 위한 요청\n",
    "answer = chain.stream({\"question\": \"미국에서 피자 주문\"})\n",
    "# 스트리밍 출력\n",
    "stream_response(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-aIqJjtZY-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
