{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API 키를 환경변수로 관리하기 위한 설정 파일\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API 키 정보 로드\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain of Density: https://arxiv.org/pdf/2309.04269.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import textwrap\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "# Load some data to summarize\n",
    "loader = WebBaseLoader(\"https://teddylee777.github.io/data-science/optuna/\")\n",
    "docs = loader.load()\n",
    "content = docs[0].page_content\n",
    "\n",
    "# Get this prompt template\n",
    "prompt = hub.pull(\"lawwu/chain_of_density\")\n",
    "\n",
    "# The chat model output is a JSON list of dicts, with SimpleJsonOutputParser\n",
    "# we can convert it o a dict, and it suppors streaming.\n",
    "json_parser = SimpleJsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"ARTICLE\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0.1)\n",
    "    | json_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Missing_Entities': 'Optuna',\n",
       "  'Denser_Summary': 'This article discusses Optuna, a hyperparameter optimization library for machine learning. Optuna provides a simple and efficient way to search for the best hyperparameters of a model. It uses a variety of search algorithms to explore the hyperparameter space and find the optimal values. With Optuna, you can easily tune the hyperparameters of your machine learning models and improve their performance.'},\n",
       " {'Missing_Entities': 'trial.suggest_categorical()',\n",
       "  'Denser_Summary': 'Optuna provides a wide range of functions to suggest hyperparameters, including trial.suggest_categorical(). This function allows you to specify a categorical parameter and its possible choices. Optuna will then explore the different choices and find the best one for your model. By using trial.suggest_categorical(), you can easily optimize categorical hyperparameters and improve the performance of your machine learning models.'},\n",
       " {'Missing_Entities': 'trial.suggest_int()',\n",
       "  'Denser_Summary': 'Another useful function provided by Optuna is trial.suggest_int(). This function allows you to specify an integer parameter and its range of possible values. Optuna will then search for the best integer value within the specified range. By using trial.suggest_int(), you can optimize integer hyperparameters and find the optimal values for your machine learning models.'},\n",
       " {'Missing_Entities': 'trial.suggest_float()',\n",
       "  'Denser_Summary': 'In addition to categorical and integer hyperparameters, Optuna also supports continuous hyperparameters with the trial.suggest_float() function. This function allows you to specify a floating-point parameter and its range of possible values. Optuna will then search for the best floating-point value within the specified range. By using trial.suggest_float(), you can optimize continuous hyperparameters and achieve better performance in your machine learning models.'},\n",
       " {'Missing_Entities': 'objective function',\n",
       "  'Denser_Summary': 'The objective function is a key component in Optuna. It defines the evaluation metric that Optuna uses to assess the performance of different hyperparameter configurations. By defining a custom objective function, you can specify the metric that is most important for your machine learning task. Optuna will then search for the hyperparameters that maximize or minimize this objective function, depending on the optimization goal. By carefully designing your objective function, you can achieve better results with Optuna.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "    {\n",
      "        \"Missing_Entities\": \"\",\n",
      "        \"Denser_Summary\": \"이 기사는 데이터사이언스, 머신러닝, 인공지능에 대한 개념을 설명하고 있습니다. 박정현 서울대 EPM 연구원이 작성한 이 글은 이러한 기술들이 우리 생활에 어떻게 적용되고 있는지, 그리고 이 분야를 공부하고자 하는 사람들에게 어떤 지식이 필요한지에 대해 다루고 있습니다. 또한, 데이터사이언스가 가장 넓은 범위를 가지며, 머신러닝과 인공지능이 어떻게 서로 관련되어 있는지 설명합니다. 이 글은 이 분야에 대한 기본적인 이해를 돕기 위해 여러 가지 예시와 함께 이론적, 실용적 측면에서 접근하고 있습니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"데이터사이언스 대학원; '인공지능 국가전략'\",\n",
      "        \"Denser_Summary\": \"박정현 서울대 EPM 연구원은 데이터사이언스, 머신러닝, 인공지능의 개념과 실생활 적용 예를 설명합니다. 데이터사이언스가 가장 넓은 범위를 가지며, 이 분야의 공부가 필요한 이유와 '인공지능 국가전략', 데이터사이언스 대학원의 설립 배경을 다룹니다. 또한, 머신러닝의 지도학습과 비지도 학습의 차이점을 설명하며, 인공지능의 약한 인공지능과 강한 인공지능으로의 분류를 소개합니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"튜링테스트; 지도학습; 비지도 학습\",\n",
      "        \"Denser_Summary\": \"박정현 연구원은 데이터사이언스, 머신러닝, 인공지능의 정의와 적용을 설명하며, '인공지능 국가전략'과 데이터사이언스 대학원 설립을 언급합니다. 데이터사이언스가 가장 넓은 범위를 가지며, 튜링테스트를 통한 인공지능의 이해, 지도학습과 비지도 학습의 머신러닝 분류 방법을 소개합니다. 이 글은 이 분야의 기초적인 이해를 위한 다양한 정보를 제공합니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"알파고; GPT-3\",\n",
      "        \"Denser_Summary\": \"박정현 연구원은 데이터사이언스, 머신러닝, 인공지능의 정의와 적용, '인공지능 국가전략', 데이터사이언스 대학원 설립을 다룹니다. 데이터사이언스가 가장 넓은 범위를 가지며, 튜링테스트, 지도학습, 비지도 학습을 통한 인공지능 이해를 설명합니다. 알파고와 GPT-3 같은 프로젝트 예시를 들며, 이 분야의 기초적인 이해를 위한 정보를 제공합니다.\"\n",
      "    },\n",
      "    {\n",
      "        \"Missing_Entities\": \"데이터 피처 엔지니어링; 알고리즘\",\n",
      "        \"Denser_Summary\": \"박정현 연구원은 데이터사이언스, 머신러닝, 인공지능의 정의와 적용, '인공지능 국가전략', 데이터사이언스 대학원 설립을 다루며, 데이터사이언스의 범위, 튜링테스트, 지도학습, 비지도 학습을 설명합니다. 알파고, GPT-3 예시와 함께 데이터 피처 엔지니어링, 알고리즘의 중요성을 강조하며, 이 분야의 기초적인 이해를 위한 정보를 제공합니다.\"\n",
      "    }\n",
      "]\n",
      "```박정현 연구원은 데이터사이언스, 머신러닝, 인공지능의 정의와 적용, '인공지능 국가전략', 데이터사이언스 대학원 설립을 다루며, 데이터사이언스의 범위, 튜링테스트, 지도학습, 비지도 학습을 설명합니다. 알파고, GPT-3 예시와 함께 데이터 피처 엔지니어링, 알고리즘의 중요성을 강조하며, 이 분야의 기초적인 이해를 위한 정보를 제공합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import json\n",
    "\n",
    "\n",
    "# Load some data to summarize\n",
    "loader = WebBaseLoader(\n",
    "    \"https://www.aitimes.com/news/articleView.html?idxno=131777\")\n",
    "docs = loader.load()\n",
    "content = docs[0].page_content\n",
    "# Load the prompt\n",
    "# prompt = hub.pull(\"langchain-ai/chain-of-density:4f55305e\")\n",
    "\n",
    "\n",
    "class StreamCallback(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Article: {ARTICLE}\n",
    "You will generate increasingly concise, entity-dense summaries of the above article. \n",
    "\n",
    "Repeat the following 2 steps 5 times. \n",
    "\n",
    "Step 1. Identify 1-3 informative entities (\";\" delimited) from the article which are missing from the previously generated summary. \n",
    "Step 2. Write a new, denser summary of identical length which covers every entity and detail from the previous summary plus the missing entities. \n",
    "\n",
    "A missing entity is:\n",
    "- relevant to the main story, \n",
    "- specific yet concise (50 words or fewer), \n",
    "- novel (not in the previous summary), \n",
    "- faithful (present in the article), \n",
    "- anywhere (can be located anywhere in the article).\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "- The first summary should be long (8-10 sentences, ~200 words) yet highly non-specific, containing little information beyond the entities marked as missing. Use overly verbose language and fillers (e.g., \"this article discusses\") to reach ~200 words.\n",
    "- Make every word count: rewrite the previous summary to improve flow and make space for additional entities.\n",
    "- Make space with fusion, compression, and removal of uninformative phrases like \"the article discusses\".\n",
    "- The summaries should become highly dense and concise yet self-contained, i.e., easily understood without the article. \n",
    "- Missing entities can appear anywhere in the new summary.\n",
    "- Never drop entities from the previous summary. If space cannot be made, add fewer new entities. \n",
    "\n",
    "Remember, use the exact same number of words for each summary.\n",
    "Answer in JSON. The JSON should be a list (length 5) of dictionaries whose keys are \"Missing_Entities\" and \"Denser_Summary\".\n",
    "Use only KOREAN language to reply.\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Create the chain, including\n",
    "chain = (\n",
    "    prompt\n",
    "    | ChatOpenAI(\n",
    "        temperature=0,\n",
    "        model=\"gpt-4-turbo-preview\",\n",
    "        streaming=True,\n",
    "        callbacks=[StreamCallback()],\n",
    "    )\n",
    "    | JsonOutputParser()\n",
    "    | (lambda x: x[-1][\"Denser_Summary\"])\n",
    ")\n",
    "\n",
    "# Invoke the chain\n",
    "result = chain.invoke({\"ARTICLE\": content})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-aIqJjtZY-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
